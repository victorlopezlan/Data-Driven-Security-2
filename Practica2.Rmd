---
title: "Practica2"
author: "Miriam, Oriol, Ulises, Víctor"
date: "2026-01-12"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F)
knitr::opts_chunk$set(warning = F)
knitr::opts_chunk$set(error = F)
knitr::opts_chunk$set(message = F)
```



Instalamos las librerías necesarias para hacer la práctica:
library(httr)
library(XML)
library(ggplot2)
library(gridExtra)

```{r carrega_llibreries}
library(httr2)
library(XML)
library(ggplot2)
library(gridExtra)
```

## **Ejercicio 1.1**

**Descargar la página web de la URL indicada, y almacenarlo en un formato de R apto para ser tratado.El primer paso para realizar tareas de crawling y scraping es poder descargar los datos de la web. Para esto usaremos la capacidad de R y de sus librerías (httr y XML) para descargar webs y almacenarlas en variables que podamos convertir en un formato fácil de analizar (p.e. de HTML a XML).**


```{r descarrega_web}
#Descargamos la web con GET y la almacenamos en una variable.
get_response <- httr::GET(url = "https://www.mediawiki.org/wiki/MediaWiki")

#Convertimos el Response estandar anterior en un XML
web_xml <- XML::htmlParse(get_response)
web_xml
```

## **Ejercicio 1.2**

**Analizar el contenido de la web, buscando el título de la página (que en HTML se etiqueta como “title”). En las cabeceras web encontramos información como el título, los ficheros de estilo visual, y meta-información como el nombre del autor de la página, una descripción de esta, el tipo de codificación de esta, o palabras clave que indican qué tipo de información contiene la página. Una vez descargada la página, y convertida a un formato analizable (como XML), buscaremos los elementos de tipo title.**

```{r analizar_web}
#Buscamos el elemento tipo title de la página web
title <- XML::xpathSApply(web_xml, "//title",xmlValue)
title

#TODO -> Limpiar el title


```


